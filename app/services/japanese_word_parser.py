"""
æ—¥è¯­å•è¯åˆ†æå™¨ - ç”Ÿäº§ç‰ˆï¼ˆæ— è°ƒè¯•ä¿¡æ¯ï¼‰
æ”¯æŒè¯å…¸æŸ¥è¯¢ã€åŠ¨è¯å˜å½¢åˆ†æã€è‡ªåŠ¨è¿˜åŸå˜å½¢è¯
"""
import json
from typing import Dict, List, Optional
from sudachipy import tokenizer, dictionary


class JapaneseWordParser:
    """æ—¥è¯­å•è¯è§£æå™¨"""
    
    def __init__(self):
        # å°è¯•å¯¼å…¥ä¾èµ–åº“
        self.jamdict = self._import_jamdict()
        self.sudachi = self._import_sudachi()
    
    def _import_jamdict(self):
        """å¯¼å…¥ jamdict è¯å…¸åº“"""
        try:
            from jamdict import Jamdict
            return Jamdict()
        except ImportError:
            print("âš ï¸ jamdict æœªå®‰è£…ï¼Œè¯å…¸åŠŸèƒ½å°†å—é™")
            return None
    
    def _import_sudachi(self):
        """å¯¼å…¥ sudachi å½¢æ€åˆ†æåº“"""
        try:
            tokenizer_obj = dictionary.Dictionary().create()
            return tokenizer_obj
        except ImportError:
            print("âš ï¸ sudachipy æœªå®‰è£…ï¼Œå½¢æ€åˆ†æåŠŸèƒ½å°†å—é™")
            return None
    
    def _katakana_to_hiragana(self, text: str) -> str:
        """å°†ç‰‡å‡åè½¬æ¢ä¸ºå¹³å‡å"""
        result = []
        for char in text:
            code = ord(char)
            if 0x30A0 <= code <= 0x30FF:
                result.append(chr(code - 0x60))
            else:
                result.append(char)
        return ''.join(result)

    def _generate_reading_for_kanji_only(self, surface: str, dictionary_form: str, dictionary_reading: str) -> str:
        """
        ä¸ºæ•´ä¸ªå•è¯ç”Ÿæˆå®Œæ•´è¯»éŸ³ï¼ˆæ±‰å­—ç”¨è¯å…¸è¯»éŸ³ï¼Œå‡åç›´æ¥ä»surfaceå–ï¼‰
        
        ç®—æ³•ï¼š
        1. æ‰¾åˆ° dictionary_form ä¸­ç¬¬ä¸€ä¸ªå‡åçš„ä½ç½®
        2. æå–è¯¥ä½ç½®ä¹‹å‰çš„è¯»éŸ³ï¼ˆè¯å¹²è¯»éŸ³ï¼‰
        3. ç»„åˆï¼šè¯å¹²è¯»éŸ³ + surfaceä¸­è¯å¹²åçš„æ‰€æœ‰å‡å
        
        ç¤ºä¾‹ï¼š
        - é©šã‹ã•ã‚ŒãŸ: ãŠã©ã‚(è¯å¹²) + ã‹ã•ã‚ŒãŸ(surfaceåç¼€) = ãŠã©ã‚ã‹ã•ã‚ŒãŸ
        - é ¼ã¾ã‚ŒãŸ: ãŸã®(è¯å¹²) + ã¾ã‚ŒãŸ(surfaceåç¼€) = ãŸã®ã¾ã‚ŒãŸ
        """
        # å¦‚æœç›¸åŒï¼Œç›´æ¥è¿”å›
        if surface == dictionary_form:
            return dictionary_reading
        
        # æ‰¾åˆ° dictionary_form ä¸­ç¬¬ä¸€ä¸ªå‡åçš„ä½ç½®
        first_hira_idx = -1
        for i, char in enumerate(dictionary_form):
            if '\u3040' <= char <= '\u309f':  # å¹³å‡å
                first_hira_idx = i
                break
        
        if first_hira_idx == -1:
            # æ²¡æœ‰å‡åï¼Œå…¨æ˜¯æ±‰å­—ï¼ˆå¦‚ï¼šèª­ï¼‰
            # æ‰¾ surface ä¸­ç¬¬ä¸€ä¸ªå‡åçš„ä½ç½®
            first_hira_in_surface = -1
            for i, char in enumerate(surface):
                if '\u3040' <= char <= '\u309f':
                    first_hira_in_surface = i
                    break
            
            if first_hira_in_surface == -1:
                # surface ä¹Ÿå…¨æ˜¯æ±‰å­—
                return dictionary_reading
            else:
                # surface ä¸­æ±‰å­—åæœ‰å‡åï¼ˆå¦‚ï¼šèª­ã‚“ã ï¼‰
                return dictionary_reading + surface[first_hira_in_surface:]
        
        # dictionary_form ä¸­æœ‰å‡åï¼ˆå¤§éƒ¨åˆ†æƒ…å†µï¼‰
        # æ‰¾åˆ°è¯¥å‡ååœ¨ dictionary_reading ä¸­çš„ä½ç½®
        first_hira_char = dictionary_form[first_hira_idx]
        stem_reading_end = -1
        
        for i, char in enumerate(dictionary_reading):
            if char == first_hira_char:
                stem_reading_end = i
                break
        
        if stem_reading_end == -1:
            stem_reading_end = len(dictionary_reading)
        
        # è¯å¹²è¯»éŸ³ï¼ˆæ±‰å­—éƒ¨åˆ†çš„è¯»éŸ³ï¼‰
        stem_reading = dictionary_reading[:stem_reading_end]
        
        # surface ä¸­è¯å¹²åçš„æ‰€æœ‰å­—ç¬¦ï¼ˆåŒ…æ‹¬å‡åå˜å½¢ï¼‰
        surface_suffix = surface[first_hira_idx:]
        
        # ç»„åˆ
        return stem_reading + surface_suffix
        
    def parse(self, word: str) -> Dict:
        """
        è§£ææ—¥è¯­å•è¯ï¼ˆè¿”å›ç»Ÿä¸€æ ¼å¼ï¼‰
        
        è¿”å›æ ¼å¼ä¸ AI åˆ†æä¸€è‡´ï¼š
        {
            "translation": "ç¿»è¯‘",
            "grammar_points": [],
            "vocabulary": [],
            "special_notes": []
        }
        
        âš ï¸ æ³¨æ„ï¼šè¿”å› Dict å¯¹è±¡ï¼Œä¸æ˜¯ JSON å­—ç¬¦ä¸²ï¼
        """
        # å…ˆå°è¯•è¿˜åŸå˜å½¢è¯åˆ°åŸå‹
        original_form = self._get_original_form(word)
        search_word = original_form if original_form else word
        
        # ä½¿ç”¨ Sudachi è¿›è¡Œå½¢æ€åˆ†æï¼ˆåˆ†æåŸå§‹è¾“å…¥ï¼‰
        morphology = self._analyze_with_sudachi(word) if self.sudachi else None
        
        # ä½¿ç”¨ Jamdict æŸ¥è¯¢è¯å…¸ï¼ˆæŸ¥è¯¢åŸå‹ï¼‰
        dict_results = self._lookup_dict(search_word) if self.jamdict else None
        
        # å¦‚æœåŸå‹æŸ¥ä¸åˆ°ï¼Œå°è¯•æŸ¥è¯¢åŸå§‹è¾“å…¥
        if not dict_results and original_form:
            dict_results = self._lookup_dict(word) if self.jamdict else None
        
        # æ„å»ºç»Ÿä¸€æ ¼å¼çš„ç»“æœ
        result = self._build_unified_result(word, morphology, dict_results)
        
        # å¦‚æœä½¿ç”¨äº†åŸå‹æŸ¥è¯¢ï¼Œæ·»åŠ æç¤º
        if original_form and original_form != word:
            result['special_notes'].insert(0, f"ğŸ’¡ å·²è‡ªåŠ¨æŸ¥è¯¢åŸå‹ï¼š{original_form}")
        
        # âœ… è¿”å› Dict å¯¹è±¡ï¼Œä¸è¦è½¬æˆ JSON å­—ç¬¦ä¸²ï¼
        return result
    
    def _get_original_form(self, word: str) -> Optional[str]:
        """è·å–å•è¯çš„åŸå‹ï¼ˆè¾ä¹¦å½¢ï¼‰"""
        if not self.sudachi:
            return None
        
        try:
            from sudachipy import tokenizer
            tokens = self.sudachi.tokenize(word, tokenizer.Tokenizer.SplitMode.C)
            
            if tokens:
                dictionary_form = tokens[0].dictionary_form()
                if dictionary_form != word:
                    return dictionary_form
            
            return None
        except Exception as e:
            return None
    
    def _analyze_with_sudachi(self, word: str) -> Optional[Dict]:
        """ä½¿ç”¨ Sudachi è¿›è¡Œå½¢æ€åˆ†æ"""
        try:
            from sudachipy import tokenizer
            
            tokens = self.sudachi.tokenize(word, tokenizer.Tokenizer.SplitMode.C)
            
            if not tokens:
                return None
            
            token = tokens[0]
            pos_tags = token.part_of_speech()
            
            # è·å–åŸå‹è¯»éŸ³ï¼ˆç‰‡å‡åï¼‰
            surface_reading = token.reading_form()
            # è½¬æ¢ä¸ºå¹³å‡å
            surface_reading = self._katakana_to_hiragana(surface_reading)
            
            return {
                'surface': token.surface(),
                'dictionary_form': token.dictionary_form(),
                'reading': surface_reading,
                'normalized_form': token.normalized_form(),
                'pos': pos_tags,
                'pos_type': self._classify_pos(pos_tags),
                'verb_type': self._get_verb_type(pos_tags),
                'verb_form': self._get_verb_form(pos_tags),
            }
        except Exception as e:
            return None
    
    def _classify_pos(self, pos_tags: List[str]) -> str:
        """åˆ†ç±»è¯æ€§"""
        if not pos_tags:
            return 'unknown'
        
        main_pos = pos_tags[0]
        
        if main_pos == 'å‹•è©':
            return 'verb'
        elif main_pos == 'å½¢å®¹è©':
            return 'i_adjective'
        elif main_pos == 'å½¢çŠ¶è©':
            return 'na_adjective'
        elif main_pos == 'åè©':
            return 'noun'
        elif main_pos == 'å‰¯è©':
            return 'adverb'
        else:
            return main_pos
    
    def _get_verb_type(self, pos_tags: List[str]) -> Optional[Dict]:
        """è·å–åŠ¨è¯ç±»å‹"""
        if len(pos_tags) < 2 or pos_tags[0] != 'å‹•è©':
            return None
        
        transitivity = pos_tags[1] if len(pos_tags) > 1 else ''
        conjugation = pos_tags[4] if len(pos_tags) > 4 else ''
        
        verb_info = {
            'transitivity': transitivity,
            'conjugation_type': conjugation
        }
        
        # åˆ¤æ–­åŠ¨è¯ç±»å‹
        if 'äº”æ®µ' in conjugation:
            verb_info['class'] = 'äº”æ®µåŠ¨è¯ï¼ˆä¸€ç±»åŠ¨è¯ï¼‰'
        elif 'ä¸€æ®µ' in conjugation:
            verb_info['class'] = 'ä¸€æ®µåŠ¨è¯ï¼ˆäºŒç±»åŠ¨è¯ï¼‰'
        elif 'ã‚µè¡Œå¤‰æ ¼' in conjugation:
            verb_info['class'] = 'ã‚µå˜åŠ¨è¯'
        elif 'ã‚«è¡Œå¤‰æ ¼' in conjugation:
            verb_info['class'] = 'ã‚«å˜åŠ¨è¯'
        else:
            verb_info['class'] = conjugation
        
        return verb_info
    
    def _get_verb_form(self, pos_tags: List[str]) -> Optional[str]:
        """è·å–åŠ¨è¯å½¢å¼"""
        if len(pos_tags) < 6 or pos_tags[0] != 'å‹•è©':
            return None
        
        return pos_tags[5] if len(pos_tags) > 5 else 'çµ‚æ­¢å½¢-ä¸€èˆ¬'
    
    def _lookup_dict(self, word: str) -> Optional[List]:
        """æŸ¥è¯¢ Jamdict è¯å…¸"""
        try:
            result = self.jamdict.lookup(word)
            entries = []
            
            for entry in result.entries:
                meanings = []
                
                for sense in entry.senses:
                    gloss_list = []
                    for gloss in sense.gloss:
                        gloss_list.append(str(gloss))
                    
                    meanings.append({
                        'pos': ', '.join([str(p) for p in sense.pos]),
                        'meanings': gloss_list
                    })
                
                readings = []
                for kana in entry.kana_forms:
                    readings.append(str(kana))
                
                entries.append({
                    'kanji': str(entry.kanji_forms[0]) if entry.kanji_forms else word,
                    'readings': readings,
                    'meanings': meanings
                })
            
            return entries if entries else None
            
        except Exception as e:
            return None
    
    def _build_unified_result(self, word: str, morphology: Optional[Dict], dict_results: Optional[List]) -> Dict:
        """æ„å»ºç»Ÿä¸€æ ¼å¼çš„ç»“æœ"""
        translation = self._build_translation(dict_results)
        vocabulary = self._build_vocabulary(word, morphology, dict_results)
        grammar_points = []
        special_notes = self._build_special_notes(morphology, dict_results)
        
        return {
            "translation": translation,
            "grammar_points": grammar_points,
            "vocabulary": vocabulary,
            "special_notes": special_notes
        }
    
    def _build_translation(self, dict_results: Optional[List]) -> str:
        """æ„å»ºç¿»è¯‘"""
        if not dict_results:
            return "ï¼ˆè¯å…¸ä¸­æœªæ‰¾åˆ°è¯¥è¯ï¼‰"
        
        first_entry = dict_results[0]
        if first_entry['meanings']:
            first_meanings = first_entry['meanings'][0]['meanings']
            return 'ã€'.join(first_meanings[:3])
        
        return "ï¼ˆæ— é‡Šä¹‰ï¼‰"
    
    def _build_vocabulary(self, word: str, morphology: Optional[Dict], dict_results: Optional[List]) -> List[Dict]:
        """æ„å»ºè¯æ±‡åˆ—è¡¨"""
        vocab_list = []
        
        main_vocab = {
            "word": word,
            "reading": "",
            "meaning": "",
            "level": "N2",
            "conjugation": {
                "has_conjugation": False
            }
        }
        
        # ä»å½¢æ€åˆ†æè·å–ä¿¡æ¯
        if morphology:
            surface = morphology.get('surface', '')
            dictionary_form = morphology.get('dictionary_form', '')
            dictionary_reading = morphology.get('reading', '')
            
            # âœ… ç”Ÿæˆå®Œæ•´è¯»éŸ³ï¼ˆæ±‰å­—+å‡åï¼‰
            complete_reading = self._generate_reading_for_kanji_only(
                surface,
                dictionary_form, 
                dictionary_reading
            )
            main_vocab["reading"] = complete_reading
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯åŠ¨è¯
            if morphology.get('pos_type') == 'verb':
                verb_type = morphology.get('verb_type')
                if verb_type and isinstance(verb_type, dict):
                    main_vocab["conjugation"] = self._build_verb_conjugation(morphology)
        
        # ä»è¯å…¸è·å–é‡Šä¹‰
        if dict_results:
            first_entry = dict_results[0]
            
            if not main_vocab["reading"] and first_entry['readings']:
                main_vocab["reading"] = first_entry['readings'][0]
            
            if first_entry['meanings']:
                meanings_list = first_entry['meanings'][0]['meanings']
                main_vocab["meaning"] = 'ï¼›'.join(meanings_list[:2])
        
        vocab_list.append(main_vocab)
        return vocab_list
    
    def _build_verb_conjugation(self, morphology: Dict) -> Dict:
        """æ„å»ºåŠ¨è¯æ´»ç”¨ä¿¡æ¯"""
        verb_type_info = morphology.get('verb_type', {})
        
        if not verb_type_info or not isinstance(verb_type_info, dict):
            return {"has_conjugation": False}
        
        dictionary_form = morphology.get('dictionary_form', '')
        surface_form = morphology.get('surface', '')
        current_form = morphology.get('verb_form', 'çµ‚æ­¢å½¢-ä¸€èˆ¬')

        # âœ… æ£€æµ‹è¢«åŠ¨å½¢å’Œä½¿å½¹å½¢ï¼ˆä¼˜å…ˆæ£€æµ‹ï¼Œå› ä¸º Sudachi å¯èƒ½è¯†åˆ«é”™è¯¯ï¼‰
        if 'ã‚Œã‚‹' in surface_form or 'ã‚‰ã‚Œã‚‹' in surface_form:
            if surface_form.endswith('ãŸ'):
                current_form = 'è¢«åŠ¨å½¢-è¿‡å»'
            elif surface_form.endswith('ã¦'):
                current_form = 'è¢«åŠ¨å½¢-ã¦å½¢'
            else:
                current_form = 'è¢«åŠ¨å½¢'
        elif 'ã›ã‚‹' in surface_form or 'ã•ã›ã‚‹' in surface_form:
            if surface_form.endswith('ãŸ'):
                current_form = 'ä½¿å½¹å½¢-è¿‡å»'
            elif surface_form.endswith('ã¦'):
                current_form = 'ä½¿å½¹å½¢-ã¦å½¢'
            else:
                current_form = 'ä½¿å½¹å½¢'

        conjugation = {
            "has_conjugation": True,
            "original_form": f"{dictionary_form}ï¼ˆ{verb_type_info.get('class', 'åŠ¨è¯')}ï¼‰",
            "current_form": surface_form,
            "conjugation_type": self._translate_verb_form(current_form),
            "reason": self._explain_verb_form(current_form),
            "verb_class": verb_type_info.get('class', ''),
            "transitivity": self._translate_transitivity(verb_type_info.get('transitivity', ''))
        }

        # âœ… ç”Ÿæˆæ‰€æœ‰æ´»ç”¨å½¢
        try:
            from app.services.verb_conjugator import get_verb_conjugator
            conjugator = get_verb_conjugator()
            all_forms = conjugator.conjugate(dictionary_form, verb_type_info.get('class', ''))
            conjugation['all_forms'] = all_forms
        except Exception as e:
            pass
        
        return conjugation
    
    def _translate_verb_form(self, form: str) -> str:
        """ç¿»è¯‘åŠ¨è¯å½¢å¼åç§°"""
        form_map = {
            'çµ‚æ­¢å½¢-ä¸€èˆ¬': 'åŸå‹ï¼ˆè¾ä¹¦å½¢ï¼‰',
            'é€£ç”¨å½¢-ä¸€èˆ¬': 'è¿ç”¨å½¢',
            'é€£ç”¨å½¢-ä¿ƒéŸ³ä¾¿': 'ãŸå½¢ï¼ˆè¿‡å»å½¢ï¼‰',
            'ä»®å®šå½¢-ä¸€èˆ¬': 'å‡å®šå½¢ï¼ˆã°å½¢ï¼‰',
            'å‘½ä»¤å½¢': 'å‘½ä»¤å½¢',
            'æœªç„¶å½¢-ä¸€èˆ¬': 'æœªç„¶å½¢',
            'é€£ä½“å½¢-ä¸€èˆ¬': 'è¿ä½“å½¢',
            'è¢«åŠ¨å½¢': 'è¢«åŠ¨å½¢ï¼ˆå—èº«å½¢ï¼‰',
            'è¢«åŠ¨å½¢-è¿‡å»': 'è¢«åŠ¨å½¢çš„è¿‡å»å¼',
            'è¢«åŠ¨å½¢-ã¦å½¢': 'è¢«åŠ¨å½¢çš„ã¦å½¢',
            'ä½¿å½¹å½¢': 'ä½¿å½¹å½¢',
            'ä½¿å½¹å½¢-è¿‡å»': 'ä½¿å½¹å½¢çš„è¿‡å»å¼',
            'ä½¿å½¹å½¢-ã¦å½¢': 'ä½¿å½¹å½¢çš„ã¦å½¢',
        }
        return form_map.get(form, form)
    
    def _explain_verb_form(self, form: str) -> str:
        """è§£é‡ŠåŠ¨è¯å½¢å¼çš„ç”¨æ³•"""
        explanations = {
            'çµ‚æ­¢å½¢-ä¸€èˆ¬': 'åŸå‹ï¼Œç”¨äºç»“å¥æˆ–ä½œä¸ºè¾ä¹¦å½¢',
            'é€£ç”¨å½¢-ä¸€èˆ¬': 'ç”¨äºè¿æ¥å…¶ä»–åŠ¨è¯æˆ–åŠ©è¯',
            'é€£ç”¨å½¢-ä¿ƒéŸ³ä¾¿': 'è¿‡å»å½¢ï¼ˆãŸå½¢ï¼‰ï¼Œè¡¨ç¤ºåŠ¨ä½œå·²å®Œæˆ',
            'ä»®å®šå½¢-ä¸€èˆ¬': 'å‡å®šå½¢ï¼Œç”¨äºè¡¨è¾¾å‡è®¾æ¡ä»¶',
            'å‘½ä»¤å½¢': 'å‘½ä»¤å½¢ï¼Œç”¨äºè¡¨è¾¾å‘½ä»¤æˆ–æŒ‡ç¤º',
            'æœªç„¶å½¢-ä¸€èˆ¬': 'æœªç„¶å½¢ï¼Œç”¨äºæ¥ç»­å¦å®šåŠ©è¯ãªã„ç­‰',
            'é€£ä½“å½¢-ä¸€èˆ¬': 'è¿ä½“å½¢ï¼Œç”¨äºä¿®é¥°åè¯',
            'è¢«åŠ¨å½¢': 'è¢«åŠ¨å½¢ï¼Œè¡¨ç¤ºè¢«åŠ¨è¯­æ€',
            'è¢«åŠ¨å½¢-è¿‡å»': 'è¢«åŠ¨å½¢çš„è¿‡å»å¼ï¼ˆå¦‚ï¼šé©šã‹ã•ã‚ŒãŸï¼‰',
            'è¢«åŠ¨å½¢-ã¦å½¢': 'è¢«åŠ¨å½¢çš„ã¦å½¢ï¼ˆå¦‚ï¼šé©šã‹ã•ã‚Œã¦ï¼‰',
            'ä½¿å½¹å½¢': 'ä½¿å½¹å½¢ï¼Œè¡¨ç¤ºè®©/ä½¿æŸäººåšæŸäº‹',
            'ä½¿å½¹å½¢-è¿‡å»': 'ä½¿å½¹å½¢çš„è¿‡å»å¼',
            'ä½¿å½¹å½¢-ã¦å½¢': 'ä½¿å½¹å½¢çš„ã¦å½¢',
        }
        return explanations.get(form, 'å…·ä½“ç”¨æ³•è¯·å‚è€ƒè¯­æ³•ä¹¦')
    
    def _translate_transitivity(self, transitivity: str) -> str:
        """ç¿»è¯‘è‡ªä»–åŠ¨è¯"""
        if 'è‡ªç«‹' in transitivity:
            return 'è‡ªåŠ¨è¯'
        elif transitivity == '':
            return ''
        return transitivity
    
    def _build_special_notes(self, morphology: Optional[Dict], dict_results: Optional[List]) -> List[str]:
        """æ„å»ºç‰¹æ®Šè¯´æ˜"""
        notes = []
        
        if not dict_results:
            notes.append("âš ï¸ è¯å…¸ä¸­æœªæ‰¾åˆ°è¯¥è¯")
            notes.append("ğŸ’¡ å¯èƒ½æ˜¯ï¼š1) å˜å½¢è¯ 2) ä¸“æœ‰åè¯ 3) è¾ƒæ–°çš„è¯æ±‡")
        
        return notes


# å…¨å±€å•ä¾‹
_parser_instance = None

def get_japanese_parser():
    """è·å–æ—¥è¯­è§£æå™¨å•ä¾‹"""
    global _parser_instance
    if _parser_instance is None:
        _parser_instance = JapaneseWordParser()
    return _parser_instance