"""
æ—¥è¯­å•è¯åˆ†æå™¨ - æœ€ç»ˆç‰ˆ
ä½¿ç”¨å­—ç¬¦çº§æ˜ å°„ç®—æ³•ï¼Œç®€å•å¯é 
"""
import json
from typing import Dict, List, Optional
from sudachipy import tokenizer, dictionary


class JapaneseWordParser:
    """æ—¥è¯­å•è¯è§£æå™¨"""
    
    def __init__(self):
        self.jamdict = self._import_jamdict()
        self.sudachi = self._import_sudachi()
    
    def _import_jamdict(self):
        """å¯¼å…¥ jamdict è¯å…¸åº“"""
        try:
            from jamdict import Jamdict
            return Jamdict()
        except ImportError:
            print("âš ï¸ jamdict æœªå®‰è£…ï¼Œè¯å…¸åŠŸèƒ½å°†å—é™")
            return None
    
    def _import_sudachi(self):
        """å¯¼å…¥ sudachi å½¢æ€åˆ†æåº“"""
        try:
            tokenizer_obj = dictionary.Dictionary().create()
            return tokenizer_obj
        except ImportError:
            print("âš ï¸ sudachipy æœªå®‰è£…ï¼Œå½¢æ€åˆ†æåŠŸèƒ½å°†å—é™")
            return None

    def _get_full_reading(self, tokens):
        """å°†æ‰€æœ‰å½¢æ€ç´ çš„è¯»éŸ³æ‹¼æ¥æˆå®Œæ•´è¯»éŸ³ï¼ˆè§£å†³è¯»éŸ³ç¼ºå¤±é—®é¢˜ï¼‰"""
        readings = []
        for t in tokens:
            r = t.reading_form()
            if r != "*":
                readings.append(r)
        full = "".join(readings)
        return self._katakana_to_hiragana(full)
    
    def _katakana_to_hiragana(self, text: str) -> str:
        """å°†ç‰‡å‡åè½¬æ¢ä¸ºå¹³å‡å"""
        result = []
        for char in text:
            code = ord(char)
            if 0x30A0 <= code <= 0x30FF:
                result.append(chr(code - 0x60))
            else:
                result.append(char)
        return ''.join(result)

    def _generate_complete_reading(self, surface: str, dictionary_form: str, dictionary_reading: str) -> str:
        """
        å­—ç¬¦çº§æ˜ å°„ç®—æ³• - ç®€å•å¯é 
        
        æ­¥éª¤ï¼š
        1. éå† dictionary_formï¼Œå»ºç«‹ã€æ±‰å­—â†’è¯»éŸ³ã€‘æ˜ å°„
        2. ç”¨æ˜ å°„æ›¿æ¢ surface ä¸­çš„æ±‰å­—ï¼Œå‡åä¿æŒåŸæ ·
        """
        # å¿«é€Ÿè·¯å¾„
        if surface == dictionary_form:
            return dictionary_reading
        
        # å»ºç«‹æ±‰å­—åˆ°è¯»éŸ³çš„æ˜ å°„
        kanji_to_reading = {}
        reading_idx = 0
        
        for i, char in enumerate(dictionary_form):
            if '\u4e00' <= char <= '\u9fff':  # æ±‰å­—
                # æå–è¿™ä¸ªæ±‰å­—çš„è¯»éŸ³ï¼ˆè¯»åˆ°ä¸‹ä¸€ä¸ªå‡åä¸ºæ­¢ï¼‰
                kanji_reading = ""
                
                while reading_idx < len(dictionary_reading):
                    next_char = dictionary_reading[reading_idx]
                    
                    # æ£€æŸ¥ï¼šæ˜¯å¦é‡åˆ° dictionary_form ä¸­åç»­çš„å‡å
                    # å¦‚æœé‡åˆ°ï¼Œè¯´æ˜æ±‰å­—è¯»éŸ³ç»“æŸ
                    found_kana_in_dict = False
                    for j in range(i + 1, len(dictionary_form)):
                        if dictionary_form[j] == next_char and ('\u3040' <= next_char <= '\u309f'):
                            found_kana_in_dict = True
                            break
                    
                    if found_kana_in_dict:
                        break
                    
                    kanji_reading += next_char
                    reading_idx += 1
                
                kanji_to_reading[char] = kanji_reading
            else:
                # å‡åï¼Œåœ¨ reading ä¸­è·³è¿‡å¯¹åº”ä½ç½®
                if reading_idx < len(dictionary_reading) and dictionary_reading[reading_idx] == char:
                    reading_idx += 1
        
        # ç”¨æ˜ å°„æ›¿æ¢ surface ä¸­çš„æ±‰å­—
        result = ""
        for char in surface:
            if '\u4e00' <= char <= '\u9fff':  # æ±‰å­—
                result += kanji_to_reading.get(char, char)
            else:  # å‡åç›´æ¥ä¿ç•™
                result += char
        
        return result
        
    def parse(self, word: str) -> Dict:
        """è§£ææ—¥è¯­å•è¯"""
        original_form = self._get_original_form(word)
        search_word = original_form if original_form else word
        
        morphology = self._analyze_with_sudachi(word) if self.sudachi else None
        dict_results = self._lookup_dict(search_word) if self.jamdict else None
        
        if not dict_results and original_form:
            dict_results = self._lookup_dict(word) if self.jamdict else None
        
        result = self._build_unified_result(word, morphology, dict_results)
        
        if original_form and original_form != word:
            result['special_notes'].insert(0, f"ğŸ’¡ å·²è‡ªåŠ¨æŸ¥è¯¢åŸå‹ï¼š{original_form}")
        
        return result
    
    def _get_original_form(self, word: str) -> Optional[str]:
        """è·å–å•è¯çš„åŸå‹ï¼ˆè¾ä¹¦å½¢ï¼‰"""
        if not self.sudachi:
            return None
        
        try:
            from sudachipy import tokenizer
            tokens = self.sudachi.tokenize(word, tokenizer.Tokenizer.SplitMode.C)
            
            if tokens:
                dictionary_form = tokens[0].dictionary_form()
                if dictionary_form != word:
                    return dictionary_form
            
            return None
        except Exception as e:
            return None
    
    def _analyze_with_sudachi(self, word: str) -> Optional[Dict]:
        """ä½¿ç”¨ Sudachi è¿›è¡Œå½¢æ€åˆ†æ"""
        try:
            from sudachipy import tokenizer
            
            tokens = self.sudachi.tokenize(word, tokenizer.Tokenizer.SplitMode.C)
            
            if not tokens:
                return None
            
            token = tokens[0]
            surface = "".join(t.surface() for t in tokens)   # ç”¨å…¨éƒ¨ morphemes ç”Ÿæˆ surface
            dictionary_form = token.dictionary_form()

            # ç”¨æ–°çš„å®Œæ•´è¯»éŸ³å‡½æ•°
            surface_reading = self._get_full_reading(tokens)
            pos_tags = token.part_of_speech()
            return {
                'surface': surface,
                'dictionary_form': dictionary_form,
                'surface_reading': surface_reading,   # â† æ·»åŠ 
                'dictionary_reading': token.reading_form(),  # â† æ·»åŠ 
                'normalized_form': token.normalized_form(),
                'pos': pos_tags,
                'pos_type': self._classify_pos(pos_tags),
                'verb_type': self._get_verb_type(pos_tags),
                'verb_form': self._get_verb_form(pos_tags),
            }
        except Exception as e:
            return None
    
    def _classify_pos(self, pos_tags: List[str]) -> str:
        """åˆ†ç±»è¯æ€§"""
        if not pos_tags:
            return 'unknown'
        
        main_pos = pos_tags[0]
        
        if main_pos == 'å‹•è©':
            return 'verb'
        elif main_pos == 'å½¢å®¹è©':
            return 'i_adjective'
        elif main_pos == 'å½¢çŠ¶è©':
            return 'na_adjective'
        elif main_pos == 'åè©':
            return 'noun'
        elif main_pos == 'å‰¯è©':
            return 'adverb'
        else:
            return main_pos
    
    def _get_verb_type(self, pos_tags: List[str]) -> Optional[Dict]:
        """è·å–åŠ¨è¯ç±»å‹"""
        if len(pos_tags) < 2 or pos_tags[0] != 'å‹•è©':
            return None
        
        transitivity = pos_tags[1] if len(pos_tags) > 1 else ''
        conjugation = pos_tags[4] if len(pos_tags) > 4 else ''
        
        verb_info = {
            'transitivity': transitivity,
            'conjugation_type': conjugation
        }
        
        if 'äº”æ®µ' in conjugation:
            verb_info['class'] = 'äº”æ®µåŠ¨è¯ï¼ˆä¸€ç±»åŠ¨è¯ï¼‰'
        elif 'ä¸€æ®µ' in conjugation:
            verb_info['class'] = 'ä¸€æ®µåŠ¨è¯ï¼ˆäºŒç±»åŠ¨è¯ï¼‰'
        elif 'ã‚µè¡Œå¤‰æ ¼' in conjugation:
            verb_info['class'] = 'ã‚µå˜åŠ¨è¯'
        elif 'ã‚«è¡Œå¤‰æ ¼' in conjugation:
            verb_info['class'] = 'ã‚«å˜åŠ¨è¯'
        else:
            verb_info['class'] = conjugation
        
        return verb_info
    
    def _get_verb_form(self, pos_tags: List[str]) -> Optional[str]:
        """è·å–åŠ¨è¯å½¢å¼"""
        if len(pos_tags) < 6 or pos_tags[0] != 'å‹•è©':
            return None
        
        return pos_tags[5] if len(pos_tags) > 5 else 'çµ‚æ­¢å½¢-ä¸€èˆ¬'
    
    def _lookup_dict(self, word: str) -> Optional[List]:
        """æŸ¥è¯¢ Jamdict è¯å…¸"""
        try:
            result = self.jamdict.lookup(word)
            entries = []
            
            for entry in result.entries:
                meanings = []
                
                for sense in entry.senses:
                    gloss_list = []
                    for gloss in sense.gloss:
                        gloss_list.append(str(gloss))
                    
                    meanings.append({
                        'pos': ', '.join([str(p) for p in sense.pos]),
                        'meanings': gloss_list
                    })
                
                readings = []
                for kana in entry.kana_forms:
                    readings.append(str(kana))
                
                entries.append({
                    'kanji': str(entry.kanji_forms[0]) if entry.kanji_forms else word,
                    'readings': readings,
                    'meanings': meanings
                })
            
            return entries if entries else None
            
        except Exception as e:
            return None
    
    def _build_unified_result(self, word: str, morphology: Optional[Dict], dict_results: Optional[List]) -> Dict:
        """æ„å»ºç»Ÿä¸€æ ¼å¼çš„ç»“æœ"""
        translation = self._build_translation(dict_results)
        vocabulary = self._build_vocabulary(word, morphology, dict_results)
        grammar_points = []
        special_notes = self._build_special_notes(morphology, dict_results)
        
        return {
            "translation": translation,
            "grammar_points": grammar_points,
            "vocabulary": vocabulary,
            "special_notes": special_notes
        }
    
    def _build_translation(self, dict_results: Optional[List]) -> str:
        """æ„å»ºç¿»è¯‘"""
        if not dict_results:
            return "ï¼ˆè¯å…¸ä¸­æœªæ‰¾åˆ°è¯¥è¯ï¼‰"
        
        first_entry = dict_results[0]
        if first_entry['meanings']:
            first_meanings = first_entry['meanings'][0]['meanings']
            return 'ã€'.join(first_meanings[:3])
        
        return "ï¼ˆæ— é‡Šä¹‰ï¼‰"
    
    def _build_vocabulary(self, word: str, morphology: Optional[Dict], dict_results: Optional[List]) -> List[Dict]:
        """æ„å»ºè¯æ±‡åˆ—è¡¨"""
        vocab_list = []
        
        main_vocab = {
            "word": word,
            "reading": "",
            "meaning": "",
            "level": "N2",
            "conjugation": {
                "has_conjugation": False
            }
        }
        
        if morphology:
            surface = morphology.get('surface', '')
            dictionary_form = morphology.get('dictionary_form', '')
            surface_reading = morphology.get('surface_reading', '')

            if surface_reading:
                main_vocab["reading"] = surface_reading
            else:
                # æ‰ä½¿ç”¨ fallback mapping
                dictionary_reading = morphology.get('dictionary_reading', '')
                main_vocab["reading"] = self._generate_complete_reading(
                    surface, dictionary_form, dictionary_reading
                )
            
            if morphology.get('pos_type') == 'verb':
                verb_type = morphology.get('verb_type')
                if verb_type and isinstance(verb_type, dict):
                    main_vocab["conjugation"] = self._build_verb_conjugation(morphology)
        
        if dict_results:
            first_entry = dict_results[0]
            
            if not main_vocab["reading"] and first_entry['readings']:
                main_vocab["reading"] = first_entry['readings'][0]
            
            if first_entry['meanings']:
                meanings_list = first_entry['meanings'][0]['meanings']
                main_vocab["meaning"] = 'ï¼›'.join(meanings_list[:2])
        
        vocab_list.append(main_vocab)
        return vocab_list
    
    def _detect_verb_form_by_ending(self, surface: str, dictionary_form: str, pos_tags: List[str]) -> Optional[str]:
        """
        é«˜ç²¾åº¦åŠ¨è¯å˜å½¢åˆ¤æ–­ï¼ˆæœ€ç»ˆç‰ˆï¼‰
        surface: å®é™…çœ‹åˆ°çš„å½¢ï¼ˆé©šã‹ã•ã‚ŒãŸã€é£Ÿã¹ã‚‰ã‚Œã‚‹ç­‰ï¼‰
        dictionary_form: åŸå‹ï¼ˆé©šã‹ã™ã€é£Ÿã¹ã‚‹ç­‰ï¼‰
        pos_tags: Sudachi ç»™çš„è¯æ€§åˆ—è¡¨
        """

        # â€”â€” 1. ä½¿å½¹è¢«åŠ¨å½¢ï¼ˆã•ã›ã‚‰ã‚Œã‚‹ ç³»ï¼‰â€”â€”
        if surface.endswith("ã•ã›ã‚‰ã‚ŒãŸ"):
            return "ä½¿å½¹è¢«åŠ¨å½¢-è¿‡å»"
        if surface.endswith("ã•ã›ã‚‰ã‚Œã¦"):
            return "ä½¿å½¹è¢«åŠ¨å½¢-ã¦å½¢"
        if surface.endswith("ã•ã›ã‚‰ã‚Œã‚‹"):
            return "ä½¿å½¹è¢«åŠ¨å½¢"

        # â€”â€” 2. çº¯ä½¿å½¹å½¢ï¼ˆã•ã›ã‚‹ ç³»ï¼‰â€”â€”
        if surface.endswith("ã•ã›ãŸ"):
            return "ä½¿å½¹å½¢-è¿‡å»"
        if surface.endswith("ã•ã›ã¦"):
            return "ä½¿å½¹å½¢-ã¦å½¢"
        if surface.endswith("ã•ã›ã‚‹"):
            return "ä½¿å½¹å½¢"

        # â€”â€” 3. ã‚‰ã‚Œã‚‹ï¼šå¯èƒ½ or è¢«åŠ¨ â€”â€” 
        if surface.endswith("ã‚‰ã‚ŒãŸ"):
            return "è¢«åŠ¨å½¢-è¿‡å»"
        if surface.endswith("ã‚‰ã‚Œã¦"):
            return "è¢«åŠ¨å½¢-ã¦å½¢"
        if surface.endswith("ã‚‰ã‚Œã‚‹"):
            # å¦‚æœæ˜¯å…¸å‹ä¸€æ®µåŠ¨è¯ï¼Œä¼˜å…ˆåˆ¤ä¸ºå¯èƒ½å½¢
            if self._is_ichidan(dictionary_form):
                return "å¯èƒ½å½¢"
            return "è¢«åŠ¨å½¢"

        # â€”â€” 4. æ¯ã‚Œã‚‹ / è¦‹ãˆã‚‹ ç­‰ã€Œæœ¬æ¥å°±ä»¥ ã‚Œã‚‹ ç»“å°¾çš„ä¸€æ®µåŠ¨è¯ã€â€”â€”
        if surface == dictionary_form and dictionary_form.endswith("ã‚Œã‚‹"):
            if self._is_ichidan(dictionary_form):
                return "åŸå‹ï¼ˆä¸€æ®µåŠ¨è¯ï¼‰"

        # â€”â€” 5. ä¸€èˆ¬è¢«åŠ¨å½¢ï¼ˆå—èº«å½¢ï¼‰â€”â€”
        if surface.endswith("ã‚ŒãŸ"):
            return "è¢«åŠ¨å½¢-è¿‡å»"
        if surface.endswith("ã‚Œã¦"):
            return "è¢«åŠ¨å½¢-ã¦å½¢"
        if surface.endswith("ã‚Œã‚‹"):
            return "è¢«åŠ¨å½¢"

        # â€”â€” 6. å¦å®šå½¢ â€”â€” 
        if surface.endswith("ãªã‹ã£ãŸ"):
            return "å¦å®šå½¢-è¿‡å»"
        if surface.endswith("ãªãã¦"):
            return "å¦å®šå½¢-ã¦å½¢"
        if surface.endswith("ãªã„"):
            return "å¦å®šå½¢"

        # â€”â€” 7. æ•¬ä½“ â€”â€”  
        if surface.endswith("ã¾ã›ã‚“ã§ã—ãŸ"):
            return "å¦å®šå½¢-è¿‡å»"
        if surface.endswith("ã¾ã—ãŸ"):
            return "æ•¬ä½“è¿‡å»å½¢"
        if surface.endswith("ã¾ã™"):
            return "æ•¬ä½“å½¢"

        return None

    def _is_ichidan(self, dictionary_form: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºä¸€æ®µåŠ¨è¯ï¼ˆéå¸¸å¯é ï¼‰
        ä¸€æ®µåŠ¨è¯è§„å¾‹ï¼šå‡åè¯å¹² + ã‚‹ï¼ˆå‰ä¸€ä¸ªå¹³å‡åæ˜¯ ã„æ®µ æˆ– ãˆæ®µï¼‰
        ä¾‹ï¼šé£Ÿã¹ã‚‹ã€è¦‹ã‚‹ã€å¯ã‚‹ã€æ¯ã‚Œã‚‹
        """
        if not dictionary_form.endswith("ã‚‹"):
            return False

        if len(dictionary_form) < 2:
            return False

        prev_char = dictionary_form[-2]

        # å¹³å‡åçš„ã€Œã„æ®µ + ãˆæ®µã€
        i_e_dan = "ã„ãã—ã¡ã«ã²ã¿ã‚Šãˆã‘ã›ã¦ã­ã¸ã‚ã‚Œ"
        return prev_char in i_e_dan
    
    def _build_verb_conjugation(self, morphology: Dict) -> Dict:
        """æ„å»ºåŠ¨è¯æ´»ç”¨ä¿¡æ¯"""
        verb_type_info = morphology.get('verb_type', {})

        if not verb_type_info or not isinstance(verb_type_info, dict):
            return {"has_conjugation": False}

        dictionary_form = morphology.get('dictionary_form', '')
        surface_form = morphology.get('surface', '')
        pos_tags = morphology.get('pos') or []
        current_form = morphology.get('verb_form', 'çµ‚æ­¢å½¢-ä¸€èˆ¬')

        # âœ… ä½¿ç”¨æ–°çš„ 3 å‚æ•°ç‰ˆæœ¬ç»“å°¾åˆ¤æ–­å‡½æ•°
        detected_form = self._detect_verb_form_by_ending(
            surface_form,
            dictionary_form,
            pos_tags,
        )
        if detected_form:
            current_form = detected_form

        conjugation = {
            "has_conjugation": True,
            "original_form": f"{dictionary_form}ï¼ˆ{verb_type_info.get('class', 'åŠ¨è¯')}ï¼‰",
            "current_form": surface_form,
            "conjugation_type": self._translate_verb_form(current_form),
            "reason": self._explain_verb_form(current_form),
            "verb_class": verb_type_info.get('class', ''),
            "transitivity": self._translate_transitivity(verb_type_info.get('transitivity', ''))
        }

        # ç”Ÿæˆæ‰€æœ‰æ´»ç”¨å½¢ï¼ˆå¦‚æœä½ å·²ç»æœ‰ verb_conjugator å°±ä¼šç”¨ä¸Šï¼Œæ²¡æœ‰ä¹Ÿä¸ä¼šå´©ï¼‰
        try:
            from app.services.verb_conjugator import get_verb_conjugator
            conjugator = get_verb_conjugator()
            all_forms = conjugator.conjugate(dictionary_form, verb_type_info.get('class', ''))
            conjugation['all_forms'] = all_forms
        except Exception:
            pass

        return conjugation
    
    def _translate_verb_form(self, form: str) -> str:
        """ç¿»è¯‘åŠ¨è¯å½¢å¼åç§°"""
        form_map = {
            'çµ‚æ­¢å½¢-ä¸€èˆ¬': 'åŸå‹ï¼ˆè¾ä¹¦å½¢ï¼‰',
            'é€£ç”¨å½¢-ä¸€èˆ¬': 'è¿ç”¨å½¢',
            'é€£ç”¨å½¢-ä¿ƒéŸ³ä¾¿': 'ãŸå½¢ï¼ˆè¿‡å»å½¢ï¼‰',
            'ä»®å®šå½¢-ä¸€èˆ¬': 'å‡å®šå½¢ï¼ˆã°å½¢ï¼‰',
            'å‘½ä»¤å½¢': 'å‘½ä»¤å½¢',
            'æœªç„¶å½¢-ä¸€èˆ¬': 'æœªç„¶å½¢',
            'é€£ä½“å½¢-ä¸€èˆ¬': 'è¿ä½“å½¢',
            'è¢«åŠ¨å½¢': 'è¢«åŠ¨å½¢ï¼ˆå—èº«å½¢ï¼‰',
            'è¢«åŠ¨å½¢-è¿‡å»': 'è¢«åŠ¨å½¢çš„è¿‡å»å¼',
            'è¢«åŠ¨å½¢-ã¦å½¢': 'è¢«åŠ¨å½¢çš„ã¦å½¢',
            'ä½¿å½¹å½¢': 'ä½¿å½¹å½¢',
            'ä½¿å½¹å½¢-è¿‡å»': 'ä½¿å½¹å½¢çš„è¿‡å»å¼',
            'ä½¿å½¹å½¢-ã¦å½¢': 'ä½¿å½¹å½¢çš„ã¦å½¢',
            'å¦å®šå½¢': 'å¦å®šå½¢ï¼ˆãªã„å½¢ï¼‰',
            'å¦å®šå½¢-è¿‡å»': 'å¦å®šå½¢çš„è¿‡å»å¼',
            'å¦å®šå½¢-ã¦å½¢': 'å¦å®šå½¢çš„ã¦å½¢',
        }
        return form_map.get(form, form)
    
    def _explain_verb_form(self, form: str) -> str:
        """è§£é‡ŠåŠ¨è¯å½¢å¼çš„ç”¨æ³•"""
        explanations = {
            'çµ‚æ­¢å½¢-ä¸€èˆ¬': 'åŸå‹ï¼Œç”¨äºç»“å¥æˆ–ä½œä¸ºè¾ä¹¦å½¢',
            'é€£ç”¨å½¢-ä¸€èˆ¬': 'ç”¨äºè¿æ¥å…¶ä»–åŠ¨è¯æˆ–åŠ©è¯',
            'é€£ç”¨å½¢-ä¿ƒéŸ³ä¾¿': 'è¿‡å»å½¢ï¼ˆãŸå½¢ï¼‰ï¼Œè¡¨ç¤ºåŠ¨ä½œå·²å®Œæˆ',
            'ä»®å®šå½¢-ä¸€èˆ¬': 'å‡å®šå½¢ï¼Œç”¨äºè¡¨è¾¾å‡è®¾æ¡ä»¶',
            'å‘½ä»¤å½¢': 'å‘½ä»¤å½¢ï¼Œç”¨äºè¡¨è¾¾å‘½ä»¤æˆ–æŒ‡ç¤º',
            'æœªç„¶å½¢-ä¸€èˆ¬': 'æœªç„¶å½¢ï¼Œç”¨äºæ¥ç»­å¦å®šåŠ©è¯ãªã„ç­‰',
            'é€£ä½“å½¢-ä¸€èˆ¬': 'è¿ä½“å½¢ï¼Œç”¨äºä¿®é¥°åè¯',
            'è¢«åŠ¨å½¢': 'è¢«åŠ¨å½¢ï¼Œè¡¨ç¤ºè¢«åŠ¨è¯­æ€',
            'è¢«åŠ¨å½¢-è¿‡å»': 'è¢«åŠ¨å½¢çš„è¿‡å»å¼',
            'è¢«åŠ¨å½¢-ã¦å½¢': 'è¢«åŠ¨å½¢çš„ã¦å½¢',
            'ä½¿å½¹å½¢': 'ä½¿å½¹å½¢ï¼Œè¡¨ç¤ºè®©/ä½¿æŸäººåšæŸäº‹',
            'ä½¿å½¹å½¢-è¿‡å»': 'ä½¿å½¹å½¢çš„è¿‡å»å¼',
            'ä½¿å½¹å½¢-ã¦å½¢': 'ä½¿å½¹å½¢çš„ã¦å½¢',
            'å¦å®šå½¢': 'å¦å®šå½¢ï¼Œè¡¨ç¤ºå¦å®š',
            'å¦å®šå½¢-è¿‡å»': 'å¦å®šå½¢çš„è¿‡å»å¼',
            'å¦å®šå½¢-ã¦å½¢': 'å¦å®šå½¢çš„ã¦å½¢',
        }
        return explanations.get(form, 'å…·ä½“ç”¨æ³•è¯·å‚è€ƒè¯­æ³•ä¹¦')
    
    def _translate_transitivity(self, transitivity: str) -> str:
        """ç¿»è¯‘è‡ªä»–åŠ¨è¯"""
        if 'è‡ªç«‹' in transitivity:
            return 'è‡ªåŠ¨è¯'
        elif transitivity == '':
            return ''
        return transitivity
    
    def _build_special_notes(self, morphology: Optional[Dict], dict_results: Optional[List]) -> List[str]:
        """æ„å»ºç‰¹æ®Šè¯´æ˜"""
        notes = []
        
        if not dict_results:
            notes.append("âš ï¸ è¯å…¸ä¸­æœªæ‰¾åˆ°è¯¥è¯")
            notes.append("ğŸ’¡ å¯èƒ½æ˜¯ï¼š1) å˜å½¢è¯ 2) ä¸“æœ‰åè¯ 3) è¾ƒæ–°çš„è¯æ±‡")
        
        return notes


# å…¨å±€å•ä¾‹
_parser_instance = None

def get_japanese_parser():
    """è·å–æ—¥è¯­è§£æå™¨å•ä¾‹"""
    global _parser_instance
    if _parser_instance is None:
        _parser_instance = JapaneseWordParser()
    return _parser_instance