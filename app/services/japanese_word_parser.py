"""
æ—¥è¯­å•è¯åˆ†æå™¨ - å®Œæ•´ç‰ˆ
æ”¯æŒè¯å…¸æŸ¥è¯¢ã€åŠ¨è¯å˜å½¢åˆ†æ
"""
import json
from typing import Dict, List, Optional
from sudachipy import tokenizer, dictionary



class JapaneseWordParser:
    """æ—¥è¯­å•è¯è§£æå™¨"""
    
    def __init__(self):
        # å°è¯•å¯¼å…¥ä¾èµ–åº“
        self.jamdict = self._import_jamdict()
        self.sudachi = self._import_sudachi()
    
    def _import_jamdict(self):
        """å¯¼å…¥ jamdict è¯å…¸åº“"""
        try:
            from jamdict import Jamdict
            return Jamdict()
        except ImportError:
            print("âš ï¸ jamdict æœªå®‰è£…ï¼Œè¯å…¸åŠŸèƒ½å°†å—é™")
            print("   å®‰è£…: poetry add jamdict")
            return None
    
    def _import_sudachi(self):
        """å¯¼å…¥ sudachi å½¢æ€åˆ†æåº“"""
        try:
            tokenizer_obj = dictionary.Dictionary().create()
            return tokenizer_obj
        except ImportError:
            print("âš ï¸ sudachipy æœªå®‰è£…ï¼Œå½¢æ€åˆ†æåŠŸèƒ½å°†å—é™")
            print("   å®‰è£…: poetry add sudachipy sudachidict_core")
            return None
    
    def parse(self, word: str) -> Dict:
        """
        è§£ææ—¥è¯­å•è¯ï¼ˆè¿”å›ç»Ÿä¸€æ ¼å¼ï¼‰
        
        è¿”å›æ ¼å¼ä¸ AI åˆ†æä¸€è‡´ï¼š
        {
            "translation": "ç¿»è¯‘",
            "grammar_points": [],
            "vocabulary": [],
            "special_notes": []
        }
        """
        # ä½¿ç”¨ Sudachi è¿›è¡Œå½¢æ€åˆ†æ
        morphology = self._analyze_with_sudachi(word) if self.sudachi else None
        
        # ä½¿ç”¨ Jamdict æŸ¥è¯¢è¯å…¸
        dict_results = self._lookup_dict(word) if self.jamdict else None
        
        # æ„å»ºç»Ÿä¸€æ ¼å¼çš„ç»“æœ
        result = self._build_unified_result(word, morphology, dict_results)
        
        return json.dumps(result, ensure_ascii=False)
    
    def _analyze_with_sudachi(self, word: str) -> Optional[Dict]:
        """ä½¿ç”¨ Sudachi è¿›è¡Œå½¢æ€åˆ†æ"""
        try:
            from sudachipy import tokenizer
            
            tokens = self.sudachi.tokenize(word, tokenizer.Tokenizer.SplitMode.C)
            
            if not tokens:
                return None
            
            token = tokens[0]  # å–ç¬¬ä¸€ä¸ªtoken
            
            # è·å–è¯æ€§
            pos_tags = token.part_of_speech()
            
            return {
                'surface': token.surface(),           # è¡¨å±‚å½¢å¼
                'dictionary_form': token.dictionary_form(),  # è¾ä¹¦å½¢
                'reading': token.reading_form(),      # è¯»éŸ³
                'normalized_form': token.normalized_form(),  # æ­£è§„åŒ–å½¢å¼
                'pos': pos_tags,                      # è¯æ€§æ ‡ç­¾
                'pos_type': self._classify_pos(pos_tags),  # è¯æ€§åˆ†ç±»
                'verb_type': self._get_verb_type(pos_tags),  # åŠ¨è¯ç±»å‹
                'verb_form': self._get_verb_form(pos_tags),  # åŠ¨è¯å½¢å¼
            }
        except Exception as e:
            print(f"Sudachi åˆ†æå¤±è´¥: {e}")
            return None
    
    def _classify_pos(self, pos_tags: List[str]) -> str:
        """åˆ†ç±»è¯æ€§"""
        if not pos_tags:
            return 'unknown'
        
        main_pos = pos_tags[0]
        
        if main_pos == 'å‹•è©':
            return 'verb'
        elif main_pos == 'å½¢å®¹è©':
            return 'i_adjective'
        elif main_pos == 'å½¢çŠ¶è©':
            return 'na_adjective'
        elif main_pos == 'åè©':
            return 'noun'
        elif main_pos == 'å‰¯è©':
            return 'adverb'
        else:
            return main_pos
    
    def _get_verb_type(self, pos_tags: List[str]) -> Optional[str]:
        """è·å–åŠ¨è¯ç±»å‹"""
        if len(pos_tags) < 2 or pos_tags[0] != 'å‹•è©':
            return None
        
        # è‡ªä»–åŠ¨è¯
        transitivity = pos_tags[1] if len(pos_tags) > 1 else ''
        
        # æ´»ç”¨ç±»å‹
        conjugation = pos_tags[4] if len(pos_tags) > 4 else ''
        
        verb_info = {
            'transitivity': transitivity,  # è‡ªç«‹, éè‡ªç«‹ ç­‰
            'conjugation_type': conjugation  # äº”æ®µ-ãƒ©è¡Œ, ä¸€æ®µ-ä¸Š, ã‚µè¡Œå¤‰æ ¼ ç­‰
        }
        
        # åˆ¤æ–­äº”æ®µ/ä¸€æ®µ/ã‚«å˜/ã‚µå˜
        if 'äº”æ®µ' in conjugation:
            verb_info['class'] = 'äº”æ®µåŠ¨è¯ï¼ˆä¸€ç±»åŠ¨è¯ï¼‰'
        elif 'ä¸€æ®µ' in conjugation:
            verb_info['class'] = 'ä¸€æ®µåŠ¨è¯ï¼ˆäºŒç±»åŠ¨è¯ï¼‰'
        elif 'ã‚µè¡Œå¤‰æ ¼' in conjugation:
            verb_info['class'] = 'ã‚µå˜åŠ¨è¯'
        elif 'ã‚«è¡Œå¤‰æ ¼' in conjugation:
            verb_info['class'] = 'ã‚«å˜åŠ¨è¯'
        else:
            verb_info['class'] = conjugation
        
        return verb_info
    
    def _get_verb_form(self, pos_tags: List[str]) -> Optional[str]:
        """è·å–åŠ¨è¯å½¢å¼ï¼ˆåŸå‹ã€ã¦å½¢ã€ãŸå½¢ç­‰ï¼‰"""
        if len(pos_tags) < 6 or pos_tags[0] != 'å‹•è©':
            return None
        
        return pos_tags[5] if len(pos_tags) > 5 else 'çµ‚æ­¢å½¢-ä¸€èˆ¬'
    
    def _lookup_dict(self, word: str) -> Optional[List]:
        """æŸ¥è¯¢ Jamdict è¯å…¸"""
        try:
            result = self.jamdict.lookup(word)
            
            entries = []
            
            # æŸ¥è¯¢è¯æ¡
            for entry in result.entries:
                meanings = []
                
                # æå–ä¸­æ–‡é‡Šä¹‰
                for sense in entry.senses:
                    # Jamdict åŒ…å«å¤šè¯­è¨€ï¼Œéœ€è¦è¿‡æ»¤ä¸­æ–‡
                    gloss_list = []
                    for gloss in sense.gloss:
                        # é»˜è®¤æ˜¯è‹±æ–‡ï¼Œæˆ‘ä»¬å…ˆç”¨è‹±æ–‡
                        gloss_list.append(str(gloss))
                    
                    meanings.append({
                        'pos': ', '.join([str(p) for p in sense.pos]),
                        'meanings': gloss_list
                    })
                
                # è·å–è¯»éŸ³
                readings = []
                for kana in entry.kana_forms:
                    readings.append(str(kana))
                
                entries.append({
                    'kanji': str(entry.kanji_forms[0]) if entry.kanji_forms else word,
                    'readings': readings,
                    'meanings': meanings
                })
            
            return entries if entries else None
            
        except Exception as e:
            print(f"è¯å…¸æŸ¥è¯¢å¤±è´¥: {e}")
            return None
    
    def _build_unified_result(self, word: str, morphology: Optional[Dict], dict_results: Optional[List]) -> Dict:
        """æ„å»ºç»Ÿä¸€æ ¼å¼çš„ç»“æœ"""
        
        # åŸºç¡€ä¿¡æ¯
        translation = self._build_translation(dict_results)
        vocabulary = self._build_vocabulary(word, morphology, dict_results)
        grammar_points = self._build_grammar_points(morphology)
        special_notes = self._build_special_notes(morphology, dict_results)
        
        return {
            "translation": translation,
            "grammar_points": grammar_points,
            "vocabulary": vocabulary,
            "special_notes": special_notes
        }
    
    def _build_translation(self, dict_results: Optional[List]) -> str:
        """æ„å»ºç¿»è¯‘"""
        if not dict_results:
            return "ï¼ˆè¯å…¸ä¸­æœªæ‰¾åˆ°è¯¥è¯ï¼‰"
        
        # å–ç¬¬ä¸€ä¸ªè¯æ¡çš„ç¬¬ä¸€ä¸ªé‡Šä¹‰
        first_entry = dict_results[0]
        if first_entry['meanings']:
            first_meanings = first_entry['meanings'][0]['meanings']
            return 'ã€'.join(first_meanings[:3])  # æœ€å¤š3ä¸ªé‡Šä¹‰
        
        return "ï¼ˆæ— é‡Šä¹‰ï¼‰"
    
    def _build_vocabulary(self, word: str, morphology: Optional[Dict], dict_results: Optional[List]) -> List[Dict]:
        """æ„å»ºè¯æ±‡åˆ—è¡¨"""
        vocab_list = []
        
        # ä¸»è¯æ¡
        main_vocab = {
            "word": word,
            "reading": "",
            "meaning": "",
            "level": "N2",  # é»˜è®¤N2ï¼Œå®é™…å¯ä»¥æ ¹æ®è¯é¢‘åˆ¤æ–­
            "conjugation": {
                "has_conjugation": False
            }
        }
        
        # ä»å½¢æ€åˆ†æè·å–è¯»éŸ³å’Œè¯æ€§
        if morphology:
            main_vocab["reading"] = morphology.get('reading', '')
            
            # å¦‚æœæ˜¯åŠ¨è¯ï¼Œæ·»åŠ æ´»ç”¨ä¿¡æ¯
            if morphology['pos_type'] == 'verb':
                main_vocab["conjugation"] = self._build_verb_conjugation(morphology)
        
        # ä»è¯å…¸è·å–é‡Šä¹‰
        if dict_results:
            first_entry = dict_results[0]
            
            # è¯»éŸ³ï¼ˆå¦‚æœå½¢æ€åˆ†ææ²¡æœ‰ï¼‰
            if not main_vocab["reading"] and first_entry['readings']:
                main_vocab["reading"] = first_entry['readings'][0]
            
            # é‡Šä¹‰
            if first_entry['meanings']:
                meanings_list = first_entry['meanings'][0]['meanings']
                main_vocab["meaning"] = 'ï¼›'.join(meanings_list[:2])
        
        vocab_list.append(main_vocab)
        
        return vocab_list
    
    def _build_verb_conjugation(self, morphology: Dict) -> Dict:
        """æ„å»ºåŠ¨è¯æ´»ç”¨ä¿¡æ¯"""
        verb_type_info = morphology.get('verb_type', {})
        
        if not verb_type_info or not isinstance(verb_type_info, dict):
            return {"has_conjugation": False}
        
        dictionary_form = morphology.get('dictionary_form', '')
        surface_form = morphology.get('surface', '')
        current_form = morphology.get('verb_form', 'çµ‚æ­¢å½¢-ä¸€èˆ¬')
        
        conjugation = {
            "has_conjugation": True,
            "original_form": f"{dictionary_form}ï¼ˆ{verb_type_info.get('class', 'åŠ¨è¯')}ï¼‰",
            "current_form": surface_form,
            "conjugation_type": self._translate_verb_form(current_form),
            "reason": self._explain_verb_form(current_form),
            "verb_class": verb_type_info.get('class', ''),
            "transitivity": self._translate_transitivity(verb_type_info.get('transitivity', ''))
        }
        
        return conjugation
    
    def _translate_verb_form(self, form: str) -> str:
        """ç¿»è¯‘åŠ¨è¯å½¢å¼åç§°"""
        form_map = {
            'çµ‚æ­¢å½¢-ä¸€èˆ¬': 'åŸå‹ï¼ˆè¾ä¹¦å½¢ï¼‰',
            'é€£ç”¨å½¢-ä¸€èˆ¬': 'è¿ç”¨å½¢',
            'é€£ç”¨å½¢-ä¿ƒéŸ³ä¾¿': 'ã¦å½¢/ãŸå½¢',
            'ä»®å®šå½¢-ä¸€èˆ¬': 'å‡å®šå½¢ï¼ˆã°å½¢ï¼‰',
            'å‘½ä»¤å½¢': 'å‘½ä»¤å½¢',
            'æœªç„¶å½¢-ä¸€èˆ¬': 'æœªç„¶å½¢',
            'é€£ä½“å½¢-ä¸€èˆ¬': 'è¿ä½“å½¢',
        }
        return form_map.get(form, form)
    
    def _explain_verb_form(self, form: str) -> str:
        """è§£é‡ŠåŠ¨è¯å½¢å¼çš„ç”¨æ³•"""
        explanations = {
            'çµ‚æ­¢å½¢-ä¸€èˆ¬': 'åŸå‹ï¼Œç”¨äºç»“å¥æˆ–ä½œä¸ºè¾ä¹¦å½¢',
            'é€£ç”¨å½¢-ä¸€èˆ¬': 'ç”¨äºè¿æ¥å…¶ä»–åŠ¨è¯æˆ–åŠ©è¯',
            'é€£ç”¨å½¢-ä¿ƒéŸ³ä¾¿': 'ç”¨äºæ„æˆã¦å½¢æˆ–ãŸå½¢ï¼Œè¡¨ç¤ºåŠ¨ä½œçš„è¿æ¥æˆ–å®Œæˆ',
            'ä»®å®šå½¢-ä¸€èˆ¬': 'å‡å®šå½¢ï¼Œç”¨äºè¡¨è¾¾å‡è®¾æ¡ä»¶',
            'å‘½ä»¤å½¢': 'å‘½ä»¤å½¢ï¼Œç”¨äºè¡¨è¾¾å‘½ä»¤æˆ–æŒ‡ç¤º',
            'æœªç„¶å½¢-ä¸€èˆ¬': 'æœªç„¶å½¢ï¼Œç”¨äºæ¥ç»­å¦å®šåŠ©è¯ãªã„ç­‰',
            'é€£ä½“å½¢-ä¸€èˆ¬': 'è¿ä½“å½¢ï¼Œç”¨äºä¿®é¥°åè¯',
        }
        return explanations.get(form, 'å…·ä½“ç”¨æ³•è¯·å‚è€ƒè¯­æ³•ä¹¦')
    
    def _translate_transitivity(self, transitivity: str) -> str:
        """ç¿»è¯‘è‡ªä»–åŠ¨è¯"""
        if 'è‡ªç«‹' in transitivity:
            return 'è‡ªåŠ¨è¯'
        elif transitivity == '':
            return ''
        return transitivity
    
    def _build_grammar_points(self, morphology: Optional[Dict]) -> List[Dict]:
        """æ„å»ºè¯­æ³•ç‚¹ï¼ˆå¦‚æœæ˜¯åŠ¨è¯ï¼Œæ˜¾ç¤ºå¸¸è§å˜å½¢ï¼‰"""
        grammar_points = []
        
        if not morphology or morphology['pos_type'] != 'verb':
            return grammar_points
        
        verb_type_info = morphology.get('verb_type', {})
        dictionary_form = morphology.get('dictionary_form', '')
        
        if not dictionary_form:
            return grammar_points
        
        # ç”Ÿæˆå¸¸è§å˜å½¢ç¤ºä¾‹
        conjugations = self._generate_verb_conjugations(dictionary_form, verb_type_info)
        
        if conjugations:
            grammar_points.append({
                "pattern": "åŠ¨è¯æ´»ç”¨å½¢å¼",
                "explanation": f"è¿™æ˜¯ä¸€ä¸ª{verb_type_info.get('class', 'åŠ¨è¯')}ï¼Œä»¥ä¸‹æ˜¯å¸¸è§çš„æ´»ç”¨å½¢å¼",
                "example_in_sentence": "",
                "level": "N2",
                "is_special": False
            })
        
        return grammar_points
    
    def _generate_verb_conjugations(self, verb: str, verb_type_info: Dict) -> Dict:
        """ç”ŸæˆåŠ¨è¯å„ç§å˜å½¢ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # TODO: å®ç°å®Œæ•´çš„åŠ¨è¯æ´»ç”¨è§„åˆ™
        # è¿™é‡Œæä¾›ä¸€ä¸ªæ¡†æ¶ï¼Œå®é™…éœ€è¦æ ¹æ®åŠ¨è¯ç±»å‹ç”Ÿæˆæ­£ç¡®çš„å˜å½¢
        
        conjugations = {
            'dictionary_form': verb,
            'masu_form': 'ï¼ˆéœ€è¦å®Œæ•´å®ç°ï¼‰',
            'te_form': 'ï¼ˆéœ€è¦å®Œæ•´å®ç°ï¼‰',
            'ta_form': 'ï¼ˆéœ€è¦å®Œæ•´å®ç°ï¼‰',
            'nai_form': 'ï¼ˆéœ€è¦å®Œæ•´å®ç°ï¼‰',
            'passive_form': 'ï¼ˆéœ€è¦å®Œæ•´å®ç°ï¼‰',
            'causative_form': 'ï¼ˆéœ€è¦å®Œæ•´å®ç°ï¼‰',
            'potential_form': 'ï¼ˆéœ€è¦å®Œæ•´å®ç°ï¼‰',
            'volitional_form': 'ï¼ˆéœ€è¦å®Œæ•´å®ç°ï¼‰',
        }
        
        return conjugations
    
    def _build_special_notes(self, morphology: Optional[Dict], dict_results: Optional[List]) -> List[str]:
        """æ„å»ºç‰¹æ®Šè¯´æ˜"""
        notes = []
        
        # è¯å…¸çŠ¶æ€è¯´æ˜
        if not self.jamdict:
            notes.append("âš ï¸ æœªå®‰è£… jamdict è¯å…¸åº“ï¼Œé‡Šä¹‰åŠŸèƒ½å—é™")
            notes.append("ğŸ’¡ å®‰è£…: poetry add jamdict")
        
        if not self.sudachi:
            notes.append("âš ï¸ æœªå®‰è£… sudachipy å½¢æ€åˆ†æåº“ï¼Œåˆ†æåŠŸèƒ½å—é™")
            notes.append("ğŸ’¡ å®‰è£…: poetry add sudachipy sudachidict_core")
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°è¯æ¡
        if not dict_results:
            notes.append("ğŸ“ è¯å…¸ä¸­æœªæ‰¾åˆ°è¯¥è¯ï¼Œå¯èƒ½æ˜¯ï¼š1) ç”Ÿåƒ»è¯ 2) å˜å½¢åçš„å½¢å¼ 3) éæ ‡å‡†å†™æ³•")
        
        # åŠ¨è¯ç±»å‹è¯´æ˜
        if morphology and morphology['pos_type'] == 'verb':
            verb_type_info = morphology.get('verb_type', {})
            if isinstance(verb_type_info, dict):
                verb_class = verb_type_info.get('class', '')
                if verb_class:
                    notes.append(f"ğŸ“š è¿™æ˜¯ä¸€ä¸ª{verb_class}")
        
        return notes


# å…¨å±€å•ä¾‹
_parser_instance = None


def get_japanese_parser():
    """è·å–æ—¥è¯­è§£æå™¨å•ä¾‹"""
    global _parser_instance
    if _parser_instance is None:
        _parser_instance = JapaneseWordParser()
    return _parser_instance


# ============= ç®€åŒ–ç‰ˆï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰=============
# å¦‚æœ sudachipy å®‰è£…å¤±è´¥ï¼Œå¯ä»¥åªä½¿ç”¨è¿™ä¸ªç®€åŒ–ç‰ˆ

class JapaneseWordParserSimple:
    """æ—¥è¯­å•è¯è§£æå™¨ï¼ˆç®€åŒ–ç‰ˆ - åªç”¨ Jamdictï¼‰"""
    
    def __init__(self):
        self.jamdict = self._import_jamdict()
    
    def _import_jamdict(self):
        """å¯¼å…¥ jamdict è¯å…¸åº“"""
        try:
            from jamdict import Jamdict
            return Jamdict()
        except ImportError:
            print("âš ï¸ jamdict æœªå®‰è£…")
            return None
    
    def parse(self, word: str) -> str:
        """è§£ææ—¥è¯­å•è¯ï¼ˆè¿”å›ç»Ÿä¸€æ ¼å¼JSONï¼‰"""
        dict_results = self._lookup_dict(word) if self.jamdict else None
        result = self._build_result(word, dict_results)
        return json.dumps(result, ensure_ascii=False)
    
    def _lookup_dict(self, word: str):
        """æŸ¥è¯¢è¯å…¸ï¼ˆåŒä¸Šï¼‰"""
        try:
            result = self.jamdict.lookup(word)
            entries = []
            
            for entry in result.entries:
                meanings = []
                for sense in entry.senses:
                    gloss_list = [str(gloss) for gloss in sense.gloss]
                    pos_list = [str(p) for p in sense.pos]
                    meanings.append({
                        'pos': ', '.join(pos_list),
                        'meanings': gloss_list
                    })
                
                readings = [str(kana) for kana in entry.kana_forms]
                kanji = str(entry.kanji_forms[0]) if entry.kanji_forms else word
                
                entries.append({
                    'kanji': kanji,
                    'readings': readings,
                    'meanings': meanings
                })
            
            return entries if entries else None
        except Exception as e:
            print(f"è¯å…¸æŸ¥è¯¢å¤±è´¥: {e}")
            return None
    
    def _build_result(self, word: str, dict_results):
        """æ„å»ºç»“æœ"""
        translation = self._build_translation(dict_results)
        vocabulary = self._build_vocabulary(word, dict_results)
        special_notes = ["ğŸ’¡ å½“å‰ä½¿ç”¨ç®€åŒ–ç‰ˆï¼ˆä»…è¯å…¸æŸ¥è¯¢ï¼Œæ— å½¢æ€åˆ†æï¼‰"]
        
        return {
            "translation": translation,
            "grammar_points": [],
            "vocabulary": vocabulary,
            "special_notes": special_notes
        }
    
    def _build_translation(self, dict_results):
        """æ„å»ºç¿»è¯‘"""
        if not dict_results:
            return "ï¼ˆè¯å…¸ä¸­æœªæ‰¾åˆ°è¯¥è¯ï¼‰"
        
        first_entry = dict_results[0]
        if first_entry['meanings']:
            first_meanings = first_entry['meanings'][0]['meanings']
            return 'ã€'.join(first_meanings[:3])
        return "ï¼ˆæ— é‡Šä¹‰ï¼‰"
    
    def _build_vocabulary(self, word: str, dict_results):
        """æ„å»ºè¯æ±‡"""
        vocab = {
            "word": word,
            "reading": "",
            "meaning": "",
            "level": "N2",
            "conjugation": {"has_conjugation": False}
        }
        
        if dict_results:
            first_entry = dict_results[0]
            if first_entry['readings']:
                vocab["reading"] = first_entry['readings'][0]
            if first_entry['meanings']:
                meanings_list = []
                for mg in first_entry['meanings'][:2]:
                    meanings_list.extend(mg['meanings'][:2])
                vocab["meaning"] = 'ï¼›'.join(meanings_list[:3])
        
        return [vocab]


def get_japanese_parser_simple():
    """è·å–ç®€åŒ–ç‰ˆè§£æå™¨ï¼ˆå¤‡ç”¨ï¼‰"""
    return JapaneseWordParserSimple()